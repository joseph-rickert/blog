<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Roland&#39;s Blog</title>
    <link>/post.html</link>
    <description>Recent content in Posts on Roland&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Why I use a fixed version of R</title>
      <link>/2019/01/05/why-i-use-a-fixed-version-of-r.html</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/05/why-i-use-a-fixed-version-of-r.html</guid>
      <description>edit
Note: As with most of my articles, discussion is linux- and cloud-oriented.
Why do I use a fixed version of R? I will keep the motivation for good versioning and reproducibility as short as possible: R projects evolve over time, as do the packages that they rely on. If you choose not to control package versions used in your R project, your code will eventually break and/or not be reproducible1.</description>
    </item>
    
    <item>
      <title>In-database xgboost Predictions with R</title>
      <link>/2018/10/18/in-database-xgboost-predictions-with-r.html</link>
      <pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/18/in-database-xgboost-predictions-with-r.html</guid>
      <description>edit
xgboost(docs) is a popular R package for classification and regression, and the model of choice in many winning Kaggle competitions. Moving xgboost into a large-scale production environment, however, can lead to challenges when attempting to calculate predictions (“scores”) for large datasets. We present a novel solution for calculating batch predictions without having to transfer features stored in a database to the machine where the model is located; instead we convert the model predictions into SQL commands and thereby transfer the scoring process to the database.</description>
    </item>
    
    <item>
      <title>Cost Effective Partitioning in BigQuery with R</title>
      <link>/2018/05/02/cost-effective-partitioning-in-bigquery-with-r.html</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/02/cost-effective-partitioning-in-bigquery-with-r.html</guid>
      <description>edit
Introduction Companies using Google BigQuery for production analytics often run into the following problem: the company has a large user hit table that spans many years. Since queries are billed based on the fields accessed, and not on the date-ranges queried, queries on the table are billed for all available days and are increasingly wasteful.
Partitioning Tables
 A solution is to partition the table by date, so that users can query a particular range of dates; saving costs and decreasing query duration.</description>
    </item>
    
  </channel>
</rss>